{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf578dbf-e0bd-4128-9908-e3fab7220b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import bigbench.api.results as bb\n",
    "\n",
    "from lass.log_handling import LogLoader, TaskLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a6e175-7f4b-4e49-b97c-12b23c2a8b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 tasks loaded\n"
     ]
    }
   ],
   "source": [
    "loader = (LogLoader(logdir = Path('../artifacts/logs'))\n",
    "        .with_tasks('paper-full')\n",
    "        .with_model_families(['BIG-G T=0'])\n",
    "        .with_model_sizes(['128b'])\n",
    "        .with_shots([3])\n",
    "        .with_query_types([bb.MultipleChoiceQuery])\n",
    ")\n",
    "\n",
    "tasks: List[bb.ResultsFileData] = list(loader.load_per_model())\n",
    "print(f\"{len(tasks)} tasks loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c35c75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337 queries\n"
     ]
    }
   ],
   "source": [
    "n_queries = sum(len(log.queries or []) for log in tasks)\n",
    "print(f\"{n_queries} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d418133d-d8df-4959-a197-c591484285c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "837f5cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>targets</th>\n",
       "      <th>scores</th>\n",
       "      <th>target_values</th>\n",
       "      <th>correct</th>\n",
       "      <th>absolute_scores</th>\n",
       "      <th>normalized_scores</th>\n",
       "      <th>metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nIn the SIT-adversarial world a structure is ...</td>\n",
       "      <td>[There is at most one yellow square.\\n, There ...</td>\n",
       "      <td>[-5.295638084411621, -4.816563129425049, -3.96...</td>\n",
       "      <td>{'There are at least two red pieces.\n",
       "': 0, 'Th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-5.295638084411621, -4.816563129425049, -3.96...</td>\n",
       "      <td>[-5.226889610290527, -4.747814655303955, -3.90...</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  \\nIn the SIT-adversarial world a structure is ...   \n",
       "\n",
       "                                             targets  \\\n",
       "0  [There is at most one yellow square.\\n, There ...   \n",
       "\n",
       "                                              scores  \\\n",
       "0  [-5.295638084411621, -4.816563129425049, -3.96...   \n",
       "\n",
       "                                       target_values  correct  \\\n",
       "0  {'There are at least two red pieces.\n",
       "': 0, 'Th...      0.0   \n",
       "\n",
       "                                     absolute_scores  \\\n",
       "0  [-5.295638084411621, -4.816563129425049, -3.96...   \n",
       "\n",
       "                                   normalized_scores  \\\n",
       "0  [-5.226889610290527, -4.747814655303955, -3.90...   \n",
       "\n",
       "                                             metrics  \n",
       "0  {'calibration_multiple_choice_brier_score': 0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0    28749\n",
       "1.0    17346\n",
       "0.8       72\n",
       "0.6       64\n",
       "Name: correct, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "train_tasks, test_tasks = train_test_split(tasks, test_size=0.2, random_state=42)\n",
    "\n",
    "def to_dataframe(tasks: List[bb.ResultsFileData]) -> pd.DataFrame:\n",
    "    dfs: List[pd.DataFrame] = []\n",
    "    for task in tasks:\n",
    "        for query in (task.queries or []):\n",
    "            dfs.append(pd.DataFrame(query.samples))\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "df_train = to_dataframe(train_tasks)\n",
    "df_test = to_dataframe(test_tasks)\n",
    "\n",
    "print(\"Original data:\")\n",
    "display(df_train.head(1))\n",
    "display(df_train['correct'].value_counts().head(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3968f8b-89b6-4baf-8bc9-14e218345bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Huggingface ready data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nIn the SIT-adversarial world a structure is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  \\nIn the SIT-adversarial world a structure is ...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    28749\n",
       "1    17346\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def huggingfaceify(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_hf = df[['input', 'correct']].rename(columns={'input':'text','correct':'label'})\n",
    "    df_hf = df_hf[df_hf['label'].isin([0.0, 1.0])]\n",
    "    df_hf[['label']] = df_hf[['label']].astype(int)\n",
    "    return df_hf\n",
    "\n",
    "\n",
    "print(\"\\n\\nHuggingface ready data:\")\n",
    "dfhf_train = huggingfaceify(df_train)\n",
    "dfhf_test = huggingfaceify(df_test)\n",
    "display(dfhf_train.head(1))\n",
    "display(dfhf_train['label'].value_counts().head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "731c85a5-b41c-405f-bb49-33dcbadf032c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train accuracy: 0.38 (46095 instances)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test accuracy: 0.33 (8994 instances)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': '\\nIn the SIT-adversarial world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 游댴 is a red circle;\\n 游릱 is a blue circle;\\n 游댮 is a yellow circle;\\n 游린 is a red triangle pointing up;\\n 游릳 is a red triangle pointing down;\\n 游댵 is a red square;\\n 游리 is a blue square;\\n _ is a yellow square;\\n 游댯 is an empty space.\\n\\nChoose the sentence consistent with the structure 游댴 游댴 游릱 游댴 游릱 游리 and not consistent with 游릳 游릳 游릱 游댮 游댯 游댯:\\n\\n  choice: There are at most two red pieces.\\n\\n  choice: There is exactly one red piece.\\n\\n  choice: There are exactly two yellow circles.\\n\\n  choice: There are at most two blue squares.\\n\\n  choice: There is at least one square.\\n\\nA: There is at least one square.\\n\\nChoose the sentence consistent with the structure _ 游댮 游릱 游댮 游댵 游리 and not consistent with 游댯 游릳 游댴 游리 _ 游댴:\\n\\n  choice: There are exactly two yellow circles.\\n\\n  choice: There is exactly one triangle.\\n\\n  choice: There are zero yellow squares.\\n\\n  choice: There are at most two blue pieces.\\n\\n  choice: There is at least one red piece.\\n\\nA: There are exactly two yellow circles.\\n\\nChoose the sentence consistent with the structure 游리 游댴 游댴 游댴 游리 游릳 and not consistent with 游댯 游릱 游댮 游댴 游릳 游린:\\n\\n  choice: There is at most one blue piece touching a blue piece.\\n\\n  choice: There is exactly one triangle at the left of a yellow circle.\\n\\n  choice: There is at most one red circle at the right of a red piece.\\n\\n  choice: There are at least two triangles pointing up touching a square.\\n\\n  choice: There is exactly one blue square touching a triangle.\\n\\nA: There is exactly one blue square touching a triangle.\\n\\nChoose the sentence consistent with the structure 游댴 游댴 游린 游댴 游댮 游댵 and not consistent with 游댴 _ 游린 游리 _ 游린:\\n\\n  choice: There are at least two triangles pointing up.\\n\\n  choice: There is at most one yellow square.\\n\\n  choice: There are at most two blue pieces.\\n\\n  choice: There are at least two red pieces.\\n\\n  choice: There are exactly two red circles.\\n\\nA: ',\n",
       " 'label': 0,\n",
       " '__index_level_0__': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "display(f\"Train accuracy: {dfhf_train['label'].mean():.2f} ({len(dfhf_train)} instances)\")\n",
    "display(f\"Test accuracy: {dfhf_test['label'].mean():.2f} ({len(dfhf_test)} instances)\")\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds['train'] = Dataset.from_pandas(dfhf_train, split='train')\n",
    "ds['test'] = Dataset.from_pandas(dfhf_test, split='test')\n",
    "\n",
    "dataset = ds\n",
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "989f9c6d-4eea-4f40-9ff9-99e74a2c5928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.635469089571542"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6642345372694248"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyse LM results\n",
    "import math\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "lm_preds = df_test[df_test['correct'].isin([0.0, 1.0])].copy()\n",
    "\n",
    "def confidence(row):\n",
    "    return np.max(row['normalized_scores'])\n",
    "\n",
    "# Assumes the actual output is arg_max\n",
    "lm_preds['confidence_normalized'] = lm_preds.apply(lambda row: math.exp(np.max(row['normalized_scores'])), axis=1)\n",
    "lm_preds['confidence_absolute'] = lm_preds.apply(lambda row: math.exp(np.max(row['absolute_scores'])), axis=1)\n",
    "\n",
    "display(sk.metrics.roc_auc_score(lm_preds['correct'], lm_preds['confidence_normalized']))\n",
    "display(sk.metrics.roc_auc_score(lm_preds['correct'], lm_preds['confidence_absolute']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73464b95-16d6-4640-ad8c-5d5994db49a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0dcb5d3713348e7acee58d93aa0e835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'torch.Tensor'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/wout/pp/lass/notebooks/EXP2_BERT_task_split_single_syst.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfalco/home/wout/pp/lass/notebooks/EXP2_BERT_task_split_single_syst.ipynb#ch0000008vscode-remote?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(examples[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m], padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfalco/home/wout/pp/lass/notebooks/EXP2_BERT_task_split_single_syst.ipynb#ch0000008vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39m# return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfalco/home/wout/pp/lass/notebooks/EXP2_BERT_task_split_single_syst.ipynb#ch0000008vscode-remote?line=7'>8</a>\u001b[0m tokenized_datasets \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(tokenize_function, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py:438\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=434'>435</a>\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=435'>436</a>\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=436'>437</a>\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=437'>438</a>\u001b[0m     {\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=438'>439</a>\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=439'>440</a>\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=440'>441</a>\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=441'>442</a>\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=442'>443</a>\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=443'>444</a>\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=444'>445</a>\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=445'>446</a>\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=446'>447</a>\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=447'>448</a>\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=448'>449</a>\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=449'>450</a>\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=450'>451</a>\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=451'>452</a>\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=452'>453</a>\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=453'>454</a>\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=454'>455</a>\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=455'>456</a>\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=456'>457</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=457'>458</a>\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=458'>459</a>\u001b[0m     }\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=459'>460</a>\u001b[0m )\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py:439\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=434'>435</a>\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=435'>436</a>\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=436'>437</a>\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=437'>438</a>\u001b[0m     {\n\u001b[0;32m--> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=438'>439</a>\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=439'>440</a>\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=440'>441</a>\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=441'>442</a>\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=442'>443</a>\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=443'>444</a>\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=444'>445</a>\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=445'>446</a>\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=446'>447</a>\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=447'>448</a>\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=448'>449</a>\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=449'>450</a>\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=450'>451</a>\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=451'>452</a>\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=452'>453</a>\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=453'>454</a>\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=454'>455</a>\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=455'>456</a>\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=456'>457</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=457'>458</a>\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=458'>459</a>\u001b[0m     }\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/dataset_dict.py?line=459'>460</a>\u001b[0m )\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py:1955\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1951'>1952</a>\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1953'>1954</a>\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1954'>1955</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1955'>1956</a>\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1956'>1957</a>\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1957'>1958</a>\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1958'>1959</a>\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1959'>1960</a>\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1960'>1961</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1961'>1962</a>\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1962'>1963</a>\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1963'>1964</a>\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1964'>1965</a>\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1965'>1966</a>\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1966'>1967</a>\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1967'>1968</a>\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1968'>1969</a>\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1969'>1970</a>\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1970'>1971</a>\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1971'>1972</a>\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1972'>1973</a>\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1973'>1974</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1974'>1975</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=1976'>1977</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py:520\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=517'>518</a>\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=518'>519</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=519'>520</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=520'>521</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=521'>522</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=522'>523</a>\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py:487\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=479'>480</a>\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=480'>481</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=481'>482</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=482'>483</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=483'>484</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=484'>485</a>\u001b[0m }\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=485'>486</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=486'>487</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=487'>488</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=488'>489</a>\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/fingerprint.py?line=451'>452</a>\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/fingerprint.py?line=452'>453</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/fingerprint.py?line=453'>454</a>\u001b[0m             )\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/fingerprint.py?line=455'>456</a>\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/fingerprint.py?line=457'>458</a>\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/fingerprint.py?line=459'>460</a>\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/fingerprint.py?line=461'>462</a>\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py:2339\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2334'>2335</a>\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2335'>2336</a>\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(input_dataset\u001b[39m.\u001b[39mnum_rows)))\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2336'>2337</a>\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2337'>2338</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2338'>2339</a>\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2339'>2340</a>\u001b[0m         batch,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2340'>2341</a>\u001b[0m         indices,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2341'>2342</a>\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(input_dataset\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2342'>2343</a>\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2343'>2344</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2344'>2345</a>\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2345'>2346</a>\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2346'>2347</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2347'>2348</a>\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py:2224\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2220'>2221</a>\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2221'>2222</a>\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2222'>2223</a>\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n\u001b[0;32m-> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2223'>2224</a>\u001b[0m     validate_function_output(processed_inputs, indices)\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2224'>2225</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m update_data:\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2225'>2226</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# Nothing to update, let's move on\u001b[39;00m\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py:2203\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.validate_function_output\u001b[0;34m(processed_inputs, indices)\u001b[0m\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2198'>2199</a>\u001b[0m all_dict_values_are_lists \u001b[39m=\u001b[39m \u001b[39mall\u001b[39m(\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2199'>2200</a>\u001b[0m     \u001b[39misinstance\u001b[39m(value, allowed_batch_return_types) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mvalues()\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2200'>2201</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2201'>2202</a>\u001b[0m \u001b[39mif\u001b[39;00m all_dict_values_are_lists \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2202'>2203</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2203'>2204</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided `function` which is applied to all elements of table returns a `dict` of types \u001b[39m\u001b[39m{\u001b[39;00m[\u001b[39mtype\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mvalues()]\u001b[39m}\u001b[39;00m\u001b[39m. When using `batched=True`, make sure provided `function` returns a `dict` of types like `\u001b[39m\u001b[39m{\u001b[39;00mallowed_batch_return_types\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/datasets/arrow_dataset.py?line=2204'>2205</a>\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'torch.Tensor'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>)`."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5705b86c-7b5f-4c0d-b707-c29e8bcacea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46072, 8986)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42) #.select(range(50))\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42) #.select(range(50))\n",
    "len(train_dataset), len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5bb053d-5306-4480-93c1-11bfd7b16a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=lass\n",
      "env: WANDB_LOG_MODEL=true\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwschella\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "%env WANDB_PROJECT=lass\n",
    "%env WANDB_LOG_MODEL=true\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c365929-0149-4813-9042-f75ea8c2ae0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766ed74bb0a145afbbe0702815d0c887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Couldn't find a directory or a metric named 'roc_auc' in this version. It was picked from the master branch on github instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import scipy\n",
    "import torch.nn as nn\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"./test_trainer/checkpoint-13500\", num_labels=2)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"../artifacts/assessors/bert-bs32/checkpoint-3000\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"xlnet-bs32-task-split\",\n",
    "    run_name=\"xlnet-bs32-task-split\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to=\"wandb\",\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": load_metric(\"accuracy\"),\n",
    "    \"precision\": load_metric(\"precision\"),\n",
    "    \"recall\": load_metric(\"recall\"),\n",
    "    \"f1\": load_metric(\"f1\"),\n",
    "    \"roc_auc\": load_metric(\"roc_auc\"),\n",
    "}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)    \n",
    "    # ROC AUC metric requires probabilities instead of logits, and only of the \"postive\" class (=\"highest label\" = 1).\n",
    "    # Needs to change for multi-class or multi-label.\n",
    "    prediction_scores = scipy.special.softmax(logits,axis=-1)[:,-1]\n",
    "    return {\n",
    "          \"accuracy\": metrics[\"accuracy\"].compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "          \"precision\": metrics[\"precision\"].compute(predictions=predictions, references=labels)[\"precision\"],\n",
    "          \"recall\": metrics[\"recall\"].compute(predictions=predictions, references=labels)[\"recall\"],\n",
    "          \"f1\": metrics[\"f1\"].compute(predictions=predictions, references=labels)[\"f1\"],\n",
    "          \"roc_auc\": metrics[\"roc_auc\"].compute(prediction_scores=prediction_scores, references=labels)[\"roc_auc\"],\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44ba8a37-0113-4e30-b7e2-a599d9f15940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `XLNetForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `XLNetForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 46072\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.17 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wout/pp/lass/notebooks/wandb/run-20220526_134141-1bul1m8c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wschella/lass/runs/1bul1m8c\" target=\"_blank\">xlnet-bs32-task-split</a></strong> to <a href=\"https://wandb.ai/wschella/lass\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 94 at dim 1 (got 215)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/wout/pp/lass/notebooks/EXP2_BERT_task_split_single_syst.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfalco/home/wout/pp/lass/notebooks/EXP2_BERT_task_split_single_syst.ipynb#ch0000012vscode-remote?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfalco/home/wout/pp/lass/notebooks/EXP2_BERT_task_split_single_syst.ipynb#ch0000012vscode-remote?line=2'>3</a>\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/transformers/trainer.py:1396\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/trainer.py?line=1392'>1393</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/trainer.py?line=1394'>1395</a>\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/trainer.py?line=1395'>1396</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/trainer.py?line=1396'>1397</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/trainer.py?line=1397'>1398</a>\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/trainer.py?line=1398'>1399</a>\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/trainer.py?line=1399'>1400</a>\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=568'>569</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=569'>570</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=570'>571</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/dataloader.py?line=571'>572</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py?line=51'>52</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py?line=59'>60</a>\u001b[0m \u001b[39m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py?line=60'>61</a>\u001b[0m \u001b[39m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py?line=61'>62</a>\u001b[0m \u001b[39m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py?line=62'>63</a>\u001b[0m \u001b[39m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py?line=64'>65</a>\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py?line=65'>66</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_default_data_collator(features)\n\u001b[1;32m     <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py?line=66'>67</a>\u001b[0m \u001b[39melif\u001b[39;00m return_tensors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py?line=67'>68</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[0;32m~/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py:130\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py?line=127'>128</a>\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([f[k] \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features])\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py?line=128'>129</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py?line=129'>130</a>\u001b[0m             batch[k] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([f[k] \u001b[39mfor\u001b[39;49;00m f \u001b[39min\u001b[39;49;00m features])\n\u001b[1;32m    <a href='file:///home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/data/data_collator.py?line=131'>132</a>\u001b[0m \u001b[39mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 94 at dim 1 (got 215)"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d51ce-e5c8-4c44-958e-c12d1e4818c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce8153-b7de-46ff-9457-fcf1f1e58090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eed4bfcf3d3cfcdb00482c10052e8eba5705b015008b357326d56e176b5397df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
