{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf578dbf-e0bd-4128-9908-e3fab7220b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from pprint import pprint\n",
    "\n",
    "import bigbench.api.results as bb\n",
    "\n",
    "from lass.log_handling import LogLoader, TaskLog\n",
    "from lass.datasets import split_task_level, analyse, merge, huggingfaceify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a6e175-7f4b-4e49-b97c-12b23c2a8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = (LogLoader(logdir = Path('../artifacts/logs'))\n",
    "        .with_tasks('paper-full')\n",
    "        .with_model_families(['BIG-G T=0'])\n",
    "        .with_model_sizes(['128b'])\n",
    "        .with_shots([3])\n",
    "        .with_query_types([bb.MultipleChoiceQuery])\n",
    ")\n",
    "\n",
    "train, test = split_task_level(loader, seed=42, test_fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0201c990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metrics': {'lm-acc': {'test': 0.3303313319991105,\n",
      "                        'train': 0.3763097949886105},\n",
      "             'lm-auc-absolute': {'test': 0.6642345372694248,\n",
      "                                 'train': 0.612106246963259},\n",
      "             'lm-auc-normalized': {'test': 0.635469089571542,\n",
      "                                   'train': 0.6041244013492464}},\n",
      " 'stats': {'n_instances': {'test': 8994, 'train': 46095},\n",
      "           'n_instances_nonbinary': {'test': 111, 'train': 231},\n",
      "           'n_tasks': {'test': 23, 'train': 95}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>targets</th>\n",
       "      <th>scores</th>\n",
       "      <th>target_values</th>\n",
       "      <th>correct</th>\n",
       "      <th>absolute_scores</th>\n",
       "      <th>normalized_scores</th>\n",
       "      <th>metrics</th>\n",
       "      <th>task</th>\n",
       "      <th>shots</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nIn the SIT-adversarial world a structure is ...</td>\n",
       "      <td>[There is at most one yellow square.\\n, There ...</td>\n",
       "      <td>[-5.295638084411621, -4.816563129425049, -3.96...</td>\n",
       "      <td>{'There are at least two red pieces.\n",
       "': 0, 'Th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[-5.295638084411621, -4.816563129425049, -3.96...</td>\n",
       "      <td>[-5.226889610290527, -4.747814655303955, -3.90...</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "      <td>symbol_interpretation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  \\nIn the SIT-adversarial world a structure is ...   \n",
       "\n",
       "                                             targets  \\\n",
       "0  [There is at most one yellow square.\\n, There ...   \n",
       "\n",
       "                                              scores  \\\n",
       "0  [-5.295638084411621, -4.816563129425049, -3.96...   \n",
       "\n",
       "                                       target_values  correct  \\\n",
       "0  {'There are at least two red pieces.\n",
       "': 0, 'Th...      0.0   \n",
       "\n",
       "                                     absolute_scores  \\\n",
       "0  [-5.295638084411621, -4.816563129425049, -3.96...   \n",
       "\n",
       "                                   normalized_scores  \\\n",
       "0  [-5.226889610290527, -4.747814655303955, -3.90...   \n",
       "\n",
       "                                             metrics                   task  \\\n",
       "0  {'calibration_multiple_choice_brier_score': 0....  symbol_interpretation   \n",
       "\n",
       "   shots  \n",
       "0      3  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = merge(analyse(train), analyse(test), 'train', 'test')\n",
    "del stats['task_names'] # Can't read anything anymore otherwise\n",
    "pprint(stats)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d418133d-d8df-4959-a197-c591484285c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3968f8b-89b6-4baf-8bc9-14e218345bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '\\nIn the SIT-adversarial world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n ğŸ”º is a red circle;\\n ğŸŸ¦ is a blue circle;\\n ğŸ”´ is a yellow circle;\\n ğŸŸ¥ is a red triangle pointing up;\\n ğŸŸ¨ is a red triangle pointing down;\\n ğŸ”» is a red square;\\n ğŸŸ¡ is a blue square;\\n _ is a yellow square;\\n ğŸ”µ is an empty space.\\n\\nChoose the sentence consistent with the structure ğŸ”º ğŸ”º ğŸŸ¦ ğŸ”º ğŸŸ¦ ğŸŸ¡ and not consistent with ğŸŸ¨ ğŸŸ¨ ğŸŸ¦ ğŸ”´ ğŸ”µ ğŸ”µ:\\n\\n  choice: There are at most two red pieces.\\n\\n  choice: There is exactly one red piece.\\n\\n  choice: There are exactly two yellow circles.\\n\\n  choice: There are at most two blue squares.\\n\\n  choice: There is at least one square.\\n\\nA: There is at least one square.\\n\\nChoose the sentence consistent with the structure _ ğŸ”´ ğŸŸ¦ ğŸ”´ ğŸ”» ğŸŸ¡ and not consistent with ğŸ”µ ğŸŸ¨ ğŸ”º ğŸŸ¡ _ ğŸ”º:\\n\\n  choice: There are exactly two yellow circles.\\n\\n  choice: There is exactly one triangle.\\n\\n  choice: There are zero yellow squares.\\n\\n  choice: There are at most two blue pieces.\\n\\n  choice: There is at least one red piece.\\n\\nA: There are exactly two yellow circles.\\n\\nChoose the sentence consistent with the structure ğŸŸ¡ ğŸ”º ğŸ”º ğŸ”º ğŸŸ¡ ğŸŸ¨ and not consistent with ğŸ”µ ğŸŸ¦ ğŸ”´ ğŸ”º ğŸŸ¨ ğŸŸ¥:\\n\\n  choice: There is at most one blue piece touching a blue piece.\\n\\n  choice: There is exactly one triangle at the left of a yellow circle.\\n\\n  choice: There is at most one red circle at the right of a red piece.\\n\\n  choice: There are at least two triangles pointing up touching a square.\\n\\n  choice: There is exactly one blue square touching a triangle.\\n\\nA: There is exactly one blue square touching a triangle.\\n\\nChoose the sentence consistent with the structure ğŸ”º ğŸ”º ğŸŸ¥ ğŸ”º ğŸ”´ ğŸ”» and not consistent with ğŸ”º _ ğŸŸ¥ ğŸŸ¡ _ ğŸŸ¥:\\n\\n  choice: There are at least two triangles pointing up.\\n\\n  choice: There is at most one yellow square.\\n\\n  choice: There are at most two blue pieces.\\n\\n  choice: There are at least two red pieces.\\n\\n  choice: There are exactly two red circles.\\n\\nA: ',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = huggingfaceify(train, test)\n",
    "dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73464b95-16d6-4640-ad8c-5d5994db49a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe043cb1a1647429e0e803fa96c5c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b669ebf80214564b302bc0f1a25fd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    # return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=1024) # longformer\n",
    "    # return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"np\") #gpt-2\n",
    "    # return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=2048) # xlnet\n",
    "\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5705b86c-7b5f-4c0d-b707-c29e8bcacea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46095, 8994)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42) #.select(range(50))\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42) #.select(range(50))\n",
    "len(train_dataset), len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5bb053d-5306-4480-93c1-11bfd7b16a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "env: WANDB_PROJECT=lass\n",
      "env: WANDB_LOG_MODEL=true\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwschella\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "%env WANDB_PROJECT=lass\n",
    "%env WANDB_LOG_MODEL=true\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c365929-0149-4813-9042-f75ea8c2ae0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Couldn't find a directory or a metric named 'roc_auc' in this version. It was picked from the master branch on github instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.auto.modeling_auto import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertModel, BertConfig\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import scipy\n",
    "import torch\n",
    "\n",
    "# model = BertModel(BertConfig.from_pretrained(\"bert-base-cased\"))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"albert-base-v2\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=2)\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"./test_trainer/checkpoint-13500\", num_labels=2)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"../artifacts/assessors/bert-bs32/checkpoint-3000\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"albert-base-v2-bs32-3sh-task-split\",\n",
    "    run_name=\"albert-base-v2-bs32-3sh-task-split\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to=\"wandb\",\n",
    "    num_train_epochs=3,\n",
    "    # max_steps=17277 - 3000,\n",
    ")\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": load_metric(\"accuracy\"),\n",
    "    \"precision\": load_metric(\"precision\"),\n",
    "    \"recall\": load_metric(\"recall\"),\n",
    "    \"f1\": load_metric(\"f1\"),\n",
    "    \"roc_auc\": load_metric(\"roc_auc\"),\n",
    "}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)    \n",
    "    # ROC AUC metric requires probabilities instead of logits, and only of the \"postive\" class (=\"highest label\" = 1).\n",
    "    # Needs to change for multi-class or multi-label.\n",
    "    prediction_scores = scipy.special.softmax(logits,axis=-1)[:,-1]\n",
    "    return {\n",
    "          \"accuracy\": metrics[\"accuracy\"].compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "          \"precision\": metrics[\"precision\"].compute(predictions=predictions, references=labels)[\"precision\"],\n",
    "          \"recall\": metrics[\"recall\"].compute(predictions=predictions, references=labels)[\"recall\"],\n",
    "          \"f1\": metrics[\"f1\"].compute(predictions=predictions, references=labels)[\"f1\"],\n",
    "          \"roc_auc\": metrics[\"roc_auc\"].compute(prediction_scores=prediction_scores, references=labels)[\"roc_auc\"],\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44ba8a37-0113-4e30-b7e2-a599d9f15940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 46095\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4323\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wout/pp/lass/notebooks/wandb/run-20220616_102545-uolqcryu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wschella/lass/runs/uolqcryu\" target=\"_blank\">albert-base-v2-bs32-3sh-task-split</a></strong> to <a href=\"https://wandb.ai/wschella/lass\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4323' max='4323' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4323/4323 51:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.625300</td>\n",
       "      <td>0.602992</td>\n",
       "      <td>0.703024</td>\n",
       "      <td>0.560778</td>\n",
       "      <td>0.465836</td>\n",
       "      <td>0.508917</td>\n",
       "      <td>0.665452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.601000</td>\n",
       "      <td>0.619904</td>\n",
       "      <td>0.608628</td>\n",
       "      <td>0.435427</td>\n",
       "      <td>0.623023</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.675861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.584500</td>\n",
       "      <td>0.580572</td>\n",
       "      <td>0.670336</td>\n",
       "      <td>0.501201</td>\n",
       "      <td>0.421407</td>\n",
       "      <td>0.457853</td>\n",
       "      <td>0.696037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to albert-base-v2-bs32-3sh-task-split/checkpoint-500\n",
      "Configuration saved in albert-base-v2-bs32-3sh-task-split/checkpoint-500/config.json\n",
      "Model weights saved in albert-base-v2-bs32-3sh-task-split/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-bs32-3sh-task-split/checkpoint-1000\n",
      "Configuration saved in albert-base-v2-bs32-3sh-task-split/checkpoint-1000/config.json\n",
      "Model weights saved in albert-base-v2-bs32-3sh-task-split/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8994\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to albert-base-v2-bs32-3sh-task-split/checkpoint-1500\n",
      "Configuration saved in albert-base-v2-bs32-3sh-task-split/checkpoint-1500/config.json\n",
      "Model weights saved in albert-base-v2-bs32-3sh-task-split/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-bs32-3sh-task-split/checkpoint-2000\n",
      "Configuration saved in albert-base-v2-bs32-3sh-task-split/checkpoint-2000/config.json\n",
      "Model weights saved in albert-base-v2-bs32-3sh-task-split/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-bs32-3sh-task-split/checkpoint-2500\n",
      "Configuration saved in albert-base-v2-bs32-3sh-task-split/checkpoint-2500/config.json\n",
      "Model weights saved in albert-base-v2-bs32-3sh-task-split/checkpoint-2500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8994\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to albert-base-v2-bs32-3sh-task-split/checkpoint-3000\n",
      "Configuration saved in albert-base-v2-bs32-3sh-task-split/checkpoint-3000/config.json\n",
      "Model weights saved in albert-base-v2-bs32-3sh-task-split/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-bs32-3sh-task-split/checkpoint-3500\n",
      "Configuration saved in albert-base-v2-bs32-3sh-task-split/checkpoint-3500/config.json\n",
      "Model weights saved in albert-base-v2-bs32-3sh-task-split/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-bs32-3sh-task-split/checkpoint-4000\n",
      "Configuration saved in albert-base-v2-bs32-3sh-task-split/checkpoint-4000/config.json\n",
      "Model weights saved in albert-base-v2-bs32-3sh-task-split/checkpoint-4000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8994\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /tmp/tmpwrnp0inv\n",
      "Configuration saved in /tmp/tmpwrnp0inv/config.json\n",
      "Model weights saved in /tmp/tmpwrnp0inv/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc25969cddc4bfb9449d00dbf8e2500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='44.592 MB of 44.592 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>â–ˆâ–â–†</td></tr><tr><td>eval/f1</td><td>â–ˆâ–ˆâ–</td></tr><tr><td>eval/loss</td><td>â–…â–ˆâ–</td></tr><tr><td>eval/precision</td><td>â–ˆâ–â–…</td></tr><tr><td>eval/recall</td><td>â–ƒâ–ˆâ–</td></tr><tr><td>eval/roc_auc</td><td>â–â–ƒâ–ˆ</td></tr><tr><td>eval/runtime</td><td>â–â–‡â–ˆ</td></tr><tr><td>eval/samples_per_second</td><td>â–ˆâ–‚â–</td></tr><tr><td>eval/steps_per_second</td><td>â–ˆâ–‚â–</td></tr><tr><td>train/epoch</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–ˆâ–ˆ</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–‡â–†â–…â–„â–ƒâ–‚â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–†â–…â–ƒâ–ƒâ–ƒâ–â–</td></tr><tr><td>train/total_flos</td><td>â–</td></tr><tr><td>train/train_loss</td><td>â–</td></tr><tr><td>train/train_runtime</td><td>â–</td></tr><tr><td>train/train_samples_per_second</td><td>â–</td></tr><tr><td>train/train_steps_per_second</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.67034</td></tr><tr><td>eval/f1</td><td>0.45785</td></tr><tr><td>eval/loss</td><td>0.58057</td></tr><tr><td>eval/precision</td><td>0.5012</td></tr><tr><td>eval/recall</td><td>0.42141</td></tr><tr><td>eval/roc_auc</td><td>0.69604</td></tr><tr><td>eval/runtime</td><td>72.3687</td></tr><tr><td>eval/samples_per_second</td><td>124.28</td></tr><tr><td>eval/steps_per_second</td><td>3.897</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>4323</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5845</td></tr><tr><td>train/total_flos</td><td>3304749001881600.0</td></tr><tr><td>train/train_loss</td><td>0.6063</td></tr><tr><td>train/train_runtime</td><td>3082.9128</td></tr><tr><td>train/train_samples_per_second</td><td>44.855</td></tr><tr><td>train/train_steps_per_second</td><td>1.402</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">albert-base-v2-bs32-3sh-task-split</strong>: <a href=\"https://wandb.ai/wschella/lass/runs/uolqcryu\" target=\"_blank\">https://wandb.ai/wschella/lass/runs/uolqcryu</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220616_102545-uolqcryu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.train()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "406d51ce-e5c8-4c44-958e-c12d1e4818c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce8153-b7de-46ff-9457-fcf1f1e58090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eed4bfcf3d3cfcdb00482c10052e8eba5705b015008b357326d56e176b5397df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
