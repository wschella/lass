{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf578dbf-e0bd-4128-9908-e3fab7220b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from pprint import pprint\n",
    "\n",
    "import bigbench.api.results as bb\n",
    "\n",
    "from lass.log_handling import LogLoader, TaskLog\n",
    "from lass.datasets import split_task_level, analyse, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a6e175-7f4b-4e49-b97c-12b23c2a8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = (LogLoader(logdir = Path('../artifacts/logs'))\n",
    "        .with_tasks('paper-full')\n",
    "        .with_model_families(['BIG-G T=0'])\n",
    "        .with_model_sizes(['128b'])\n",
    "        .with_shots([0])\n",
    "        .with_query_types([bb.MultipleChoiceQuery])\n",
    ")\n",
    "\n",
    "train, test = split_task_level(loader, seed=42, test_fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0201c990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metrics': {'lm-acc': {'test': 0.31445908841295994,\n",
      "                        'train': 0.3419742693088115}},\n",
      " 'stats': {'n_instances': {'test': 9105, 'train': 46326},\n",
      "           'n_instances_nonbinary': {'test': 119, 'train': 254},\n",
      "           'n_tasks': {'test': 23, 'train': 95}}}\n"
     ]
    }
   ],
   "source": [
    "stats = merge(analyse(train), analyse(test), 'train', 'test')\n",
    "del stats['task_names'] # Can't read anything anymore otherwise\n",
    "pprint(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c35c75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337 queries\n"
     ]
    }
   ],
   "source": [
    "n_queries = sum(len(log.queries or []) for log in tasks)\n",
    "print(f\"{n_queries} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d418133d-d8df-4959-a197-c591484285c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837f5cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tasks: ['symbol_interpretation', 'salient_translation_error_detection', 'minute_mysteries_qa', 'talkdown', 'dyck_languages', 'natural_instructions', 'language_identification', 'hinglish_toxicity', 'simp_turing_concept', 'analogical_similarity', 'misconceptions_russian', 'physics', 'suicide_risk', 'factuality_of_summary', 'international_phonetic_alphabet_nli', 'formal_fallacies_syllogisms_negation', 'conlang_translation', 'sufficient_information', 'qa_wikidata', 'movie_dialog_same_or_different', 'intersect_geometry', 'disfl_qa', 'logical_sequence', 'color', 'snarks', 'metaphor_understanding', 'human_organs_senses', 'unit_conversion', 'bias_from_probabilities', 'authorship_verification', 'crash_blossom', 'simple_ethical_questions', 'arithmetic', 'dark_humor_detection', 'matrixshapes', 'unqover', 'intent_recognition', 'bbq_lite_json', 'social_iqa', 'physics_questions', 'abstract_narrative_understanding', 'rephrase', 'polish_sequence_labeling', 'gre_reading_comprehension', 'english_proverbs', 'metaphor_boolean', 'crass_ai', 'elementary_math_qa', 'presuppositions_as_nli', 'conceptual_combinations', 'com2sense', 'scientific_press_release', 'kannada', 'irony_identification', 'language_games', 'dynamic_counting', 'mnist_ascii', 'linguistic_mappings', 'phrase_relatedness', 'what_is_the_tao', 'sports_understanding', 'entailed_polarity_hindi', 'mathematical_induction', 'similarities_abstraction', 'undo_permutation', 'general_knowledge', 'implicatures', 'date_understanding', 'play_dialog_same_or_different', 'repeat_copy_logic', 'semantic_parsing_in_context_sparc', 'geometric_shapes', 'word_sorting', 'object_counting', 'paragraph_segmentation', 'movie_recommendation', 'few_shot_nlg', 'ascii_word_recognition', 'temporal_sequences', 'analytic_entailment', 'understanding_fables', 'evaluating_information_essentiality', 'discourse_marker_prediction', 'kanji_ascii', 'international_phonetic_alphabet_transliterate', 'disambiguation_qa', 'entailed_polarity', 'auto_categorization', 'empirical_judgments', 'hyperbaton', 'ruin_names', 'nonsense_words_grammar', 'logical_deduction', 'known_unknowns', 'swedish_to_german_proverbs', 'tense', 'logic_grid_puzzle', 'auto_debugging', 'bridging_anaphora_resolution_barqa', 'gender_sensitivity_chinese', 'tracking_shuffled_objects', 'reasoning_about_colored_objects', 'chess_state_tracking', 'identify_odd_metaphor', 'swahili_english_proverbs', 'riddle_sense', 'social_support', 'topical_chat', 'goal_step_wikihow', 'figure_of_speech_detection', 'multiemo', 'fact_checker', 'winowhy', 'gender_inclusive_sentences_german', 'epistemic_reasoning', 'list_functions', 'codenames', 'gem', 'web_of_lies', 'question_selection', 'diverse_social_bias', 'unit_interpretation', 'anachronisms', 'fantasy_reasoning', 'strange_stories', 'real_or_fake_text', 'subject_verb_agreement', 'moral_permissibility', 'misconceptions', 'parsinlu_qa', 'linguistics_puzzles', 'implicit_relations', 'physical_intuition', 'word_unscrambling', 'code_line_description', 'identify_math_theorems', 'mult_data_wrangling', 'causal_judgment', 'logical_fallacy_detection', 'modified_arithmetic']\n",
      "Test tasks: ['cifar10_classification', 'english_russian_proverbs', 'semantic_parsing_spider', 'cryptonite', 'hindu_knowledge', 'checkmate_in_one', 'persian_idioms', 'which_wiki_edit', 'navigate', 'sentence_ambiguity', 'common_morpheme', 'timedial', 'emoji_movie', 'penguins_in_a_table', 'cause_and_effect', 'novel_concepts', 'odd_one_out', 'key_value_maps', 'bbq_lite', 'operators', 'chinese_remainder_theorem', 'hindi_question_answering', 'gender_sensitivity_english', 'unnatural_in_context_learning', 'vitaminc_fact_verification', 'strategyqa', 'parsinlu_reading_comprehension', 'hhh_alignment', 'logical_args', 'forecasting_subquestions', 'cryobiology_spanish', 'protein_interacting_sites', 'simple_text_editing', 'cs_algorithms', 'boolean_expressions', 'emojis_emotion_prediction']\n",
      "Original data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>targets</th>\n",
       "      <th>scores</th>\n",
       "      <th>target_values</th>\n",
       "      <th>correct</th>\n",
       "      <th>absolute_scores</th>\n",
       "      <th>normalized_scores</th>\n",
       "      <th>metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nIn the SIT-adversarial world a structure is ...</td>\n",
       "      <td>[There is at least one triangle pointing down....</td>\n",
       "      <td>[-9.594904899597168, -7.830936431884766, -8.70...</td>\n",
       "      <td>{'There are at least two blue pieces.\n",
       "': 0, 'T...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[-9.594904899597168, -7.830936431884766, -8.70...</td>\n",
       "      <td>[-2.8301148414611816, -1.0661463737487793, -1....</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  \\nIn the SIT-adversarial world a structure is ...   \n",
       "\n",
       "                                             targets  \\\n",
       "0  [There is at least one triangle pointing down....   \n",
       "\n",
       "                                              scores  \\\n",
       "0  [-9.594904899597168, -7.830936431884766, -8.70...   \n",
       "\n",
       "                                       target_values  correct  \\\n",
       "0  {'There are at least two blue pieces.\n",
       "': 0, 'T...      1.0   \n",
       "\n",
       "                                     absolute_scores  \\\n",
       "0  [-9.594904899597168, -7.830936431884766, -8.70...   \n",
       "\n",
       "                                   normalized_scores  \\\n",
       "0  [-2.8301148414611816, -1.0661463737487793, -1....   \n",
       "\n",
       "                                             metrics  \n",
       "0  {'calibration_multiple_choice_brier_score': 0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.0    30373\n",
       "1.0    15699\n",
       "0.8       85\n",
       "0.6       63\n",
       "Name: correct, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "train_tasks, test_tasks = train_test_split(tasks, test_size=0.2, random_state=42)\n",
    "print(f\"Train tasks: {[t.task.task_name for t in train_tasks]}\")\n",
    "print(f\"Test tasks: {[t.task.task_name for t in test_tasks]}\")\n",
    "\n",
    "def to_dataframe(tasks: List[bb.ResultsFileData]) -> pd.DataFrame:\n",
    "    dfs: List[pd.DataFrame] = []\n",
    "    for task in tasks:\n",
    "        for query in (task.queries or []):\n",
    "            dfs.append(pd.DataFrame(query.samples))\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "df_train = to_dataframe(train_tasks)\n",
    "df_test = to_dataframe(test_tasks)\n",
    "\n",
    "print(\"Original data:\")\n",
    "display(df_train.head(1))\n",
    "display(df_train['correct'].value_counts().head(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3968f8b-89b6-4baf-8bc9-14e218345bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Huggingface ready data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nIn the SIT-adversarial world a structure is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  \\nIn the SIT-adversarial world a structure is ...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    30373\n",
       "1    15699\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def huggingfaceify(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_hf = df[['input', 'correct']].rename(columns={'input':'text','correct':'label'})\n",
    "    df_hf = df_hf[df_hf['label'].isin([0.0, 1.0])]\n",
    "    df_hf[['label']] = df_hf[['label']].astype(int)\n",
    "    return df_hf\n",
    "\n",
    "\n",
    "print(\"\\n\\nHuggingface ready data:\")\n",
    "dfhf_train = huggingfaceify(df_train)\n",
    "dfhf_test = huggingfaceify(df_test)\n",
    "display(dfhf_train.head(1))\n",
    "display(dfhf_train['label'].value_counts().head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "731c85a5-b41c-405f-bb49-33dcbadf032c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train accuracy: 0.34 (46072 instances)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Test accuracy: 0.31 (8986 instances)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': '\\nIn the SIT-adversarial world a structure is a sequence of six emojis.\\nHereafter are reported the emojis used along with their descriptions.\\n 🔺 is a red circle;\\n 🟦 is a blue circle;\\n 🔴 is a yellow circle;\\n 🟥 is a red triangle pointing up;\\n 🟨 is a red triangle pointing down;\\n 🔻 is a red square;\\n 🟡 is a blue square;\\n _ is a yellow square;\\n 🔵 is an empty space.\\n\\nChoose the sentence consistent with the structure 🟥 🔻 🔺 🟡 🟥 🟨 and not consistent with 🔺 🟡 🟡 🟨 🟦 _:\\n\\n  choice: There are at least two triangles.\\n\\n  choice: There is at least one triangle.\\n\\n  choice: There are at least two yellow squares.\\n\\n  choice: There are at least two blue pieces.\\n\\n  choice: There is at least one triangle pointing down.\\n\\nA: ',\n",
       " 'label': 1,\n",
       " '__index_level_0__': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "display(f\"Train accuracy: {dfhf_train['label'].mean():.2f} ({len(dfhf_train)} instances)\")\n",
    "display(f\"Test accuracy: {dfhf_test['label'].mean():.2f} ({len(dfhf_test)} instances)\")\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds['train'] = Dataset.from_pandas(dfhf_train, split='train')\n",
    "ds['test'] = Dataset.from_pandas(dfhf_test, split='test')\n",
    "\n",
    "dataset = ds\n",
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "989f9c6d-4eea-4f40-9ff9-99e74a2c5928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5903410113824089"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5514382335446202"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyse LM results\n",
    "import math\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "lm_preds = df_test[df_test['correct'].isin([0.0, 1.0])].copy()\n",
    "\n",
    "def confidence(row):\n",
    "    return np.max(row['normalized_scores'])\n",
    "\n",
    "# Assumes the actual output is arg_max\n",
    "lm_preds['confidence_normalized'] = lm_preds.apply(lambda row: math.exp(np.max(row['normalized_scores'])), axis=1)\n",
    "lm_preds['confidence_absolute'] = lm_preds.apply(lambda row: math.exp(np.max(row['absolute_scores'])), axis=1)\n",
    "\n",
    "display(sk.metrics.roc_auc_score(lm_preds['correct'], lm_preds['confidence_normalized']))\n",
    "display(sk.metrics.roc_auc_score(lm_preds['correct'], lm_preds['confidence_absolute']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73464b95-16d6-4640-ad8c-5d5994db49a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b62b241ba5472c9d958fb1e85c06ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8559ef377da84d8e9c133242cf191b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/742k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca40542f8ce34214971be8cda8200bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11d3993f706485c91b1f03caf63d576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62eadb9b1a0f477780ca370a0afd97a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "    # return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=1024) # longformer\n",
    "    # return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"np\") #gpt-2\n",
    "    # return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=2048) # xlnet\n",
    "\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5705b86c-7b5f-4c0d-b707-c29e8bcacea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46072, 8986)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42) #.select(range(50))\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42) #.select(range(50))\n",
    "len(train_dataset), len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5bb053d-5306-4480-93c1-11bfd7b16a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "env: WANDB_PROJECT=lass\n",
      "env: WANDB_LOG_MODEL=true\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwschella\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "%env WANDB_PROJECT=lass\n",
    "%env WANDB_LOG_MODEL=true\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c365929-0149-4813-9042-f75ea8c2ae0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99690831bf964fdabe3d369301373061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Couldn't find a directory or a metric named 'roc_auc' in this version. It was picked from the master branch on github instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.auto.modeling_auto import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertModel, BertConfig\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "import scipy\n",
    "import torch.nn as nn\n",
    "\n",
    "# model = BertModel(BertConfig.from_pretrained(\"bert-base-cased\"))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"albert-base-v2\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=2)\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"./test_trainer/checkpoint-13500\", num_labels=2)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"../artifacts/assessors/bert-bs32/checkpoint-3000\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"albert-base-v2-bs32-task-split\",\n",
    "    run_name=\"albert-base-v2-bs32-task-split\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to=\"wandb\",\n",
    "    num_train_epochs=3,\n",
    "    # max_steps=17277 - 3000,\n",
    ")\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": load_metric(\"accuracy\"),\n",
    "    \"precision\": load_metric(\"precision\"),\n",
    "    \"recall\": load_metric(\"recall\"),\n",
    "    \"f1\": load_metric(\"f1\"),\n",
    "    \"roc_auc\": load_metric(\"roc_auc\"),\n",
    "}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)    \n",
    "    # ROC AUC metric requires probabilities instead of logits, and only of the \"postive\" class (=\"highest label\" = 1).\n",
    "    # Needs to change for multi-class or multi-label.\n",
    "    prediction_scores = scipy.special.softmax(logits,axis=-1)[:,-1]\n",
    "    return {\n",
    "          \"accuracy\": metrics[\"accuracy\"].compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "          \"precision\": metrics[\"precision\"].compute(predictions=predictions, references=labels)[\"precision\"],\n",
    "          \"recall\": metrics[\"recall\"].compute(predictions=predictions, references=labels)[\"recall\"],\n",
    "          \"f1\": metrics[\"f1\"].compute(predictions=predictions, references=labels)[\"f1\"],\n",
    "          \"roc_auc\": metrics[\"roc_auc\"].compute(prediction_scores=prediction_scores, references=labels)[\"roc_auc\"],\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44ba8a37-0113-4e30-b7e2-a599d9f15940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/wout/pp/lass/.env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 46072\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4320\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wout/pp/lass/notebooks/wandb/run-20220615_124502-3063kkvs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wschella/lass/runs/3063kkvs\" target=\"_blank\">albert-base-v2-bs32-task-split</a></strong> to <a href=\"https://wandb.ai/wschella/lass\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4320' max='4320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4320/4320 50:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.631900</td>\n",
       "      <td>0.573639</td>\n",
       "      <td>0.687180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>0.566076</td>\n",
       "      <td>0.714667</td>\n",
       "      <td>0.581411</td>\n",
       "      <td>0.313767</td>\n",
       "      <td>0.407579</td>\n",
       "      <td>0.690441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.558200</td>\n",
       "      <td>0.568525</td>\n",
       "      <td>0.702203</td>\n",
       "      <td>0.538726</td>\n",
       "      <td>0.334045</td>\n",
       "      <td>0.412385</td>\n",
       "      <td>0.694846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to albert-base-v2-bs32-task-split/checkpoint-500\n",
      "Configuration saved in albert-base-v2-bs32-task-split/checkpoint-500/config.json\n",
      "Model weights saved in albert-base-v2-bs32-task-split/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-bs32-task-split/checkpoint-1000\n",
      "Configuration saved in albert-base-v2-bs32-task-split/checkpoint-1000/config.json\n",
      "Model weights saved in albert-base-v2-bs32-task-split/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8986\n",
      "  Batch size = 32\n",
      "/home/wout/pp/lass/.env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to albert-base-v2-bs32-task-split/checkpoint-1500\n",
      "Configuration saved in albert-base-v2-bs32-task-split/checkpoint-1500/config.json\n",
      "Model weights saved in albert-base-v2-bs32-task-split/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-bs32-task-split/checkpoint-2000\n",
      "Configuration saved in albert-base-v2-bs32-task-split/checkpoint-2000/config.json\n",
      "Model weights saved in albert-base-v2-bs32-task-split/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-bs32-task-split/checkpoint-2500\n",
      "Configuration saved in albert-base-v2-bs32-task-split/checkpoint-2500/config.json\n",
      "Model weights saved in albert-base-v2-bs32-task-split/checkpoint-2500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8986\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to albert-base-v2-bs32-task-split/checkpoint-3000\n",
      "Configuration saved in albert-base-v2-bs32-task-split/checkpoint-3000/config.json\n",
      "Model weights saved in albert-base-v2-bs32-task-split/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-bs32-task-split/checkpoint-3500\n",
      "Configuration saved in albert-base-v2-bs32-task-split/checkpoint-3500/config.json\n",
      "Model weights saved in albert-base-v2-bs32-task-split/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to albert-base-v2-bs32-task-split/checkpoint-4000\n",
      "Configuration saved in albert-base-v2-bs32-task-split/checkpoint-4000/config.json\n",
      "Model weights saved in albert-base-v2-bs32-task-split/checkpoint-4000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `AlbertForSequenceClassification.forward` and have been ignored: text, __index_level_0__. If text, __index_level_0__ are not expected by `AlbertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 8986\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /tmp/tmpbcgmlffo\n",
      "Configuration saved in /tmp/tmpbcgmlffo/config.json\n",
      "Model weights saved in /tmp/tmpbcgmlffo/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4912f4e6317144d7b4be35788eae66c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='44.592 MB of 44.592 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁█▅</td></tr><tr><td>eval/f1</td><td>▁██</td></tr><tr><td>eval/loss</td><td>█▁▃</td></tr><tr><td>eval/precision</td><td>▁█▇</td></tr><tr><td>eval/recall</td><td>▁██</td></tr><tr><td>eval/roc_auc</td><td>▁▇█</td></tr><tr><td>eval/runtime</td><td>▁▁█</td></tr><tr><td>eval/samples_per_second</td><td>██▁</td></tr><tr><td>eval/steps_per_second</td><td>██▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▄▅▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▄▅▅▆▆▇██</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▃▃▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.7022</td></tr><tr><td>eval/f1</td><td>0.41238</td></tr><tr><td>eval/loss</td><td>0.56852</td></tr><tr><td>eval/precision</td><td>0.53873</td></tr><tr><td>eval/recall</td><td>0.33404</td></tr><tr><td>eval/roc_auc</td><td>0.69485</td></tr><tr><td>eval/runtime</td><td>75.6231</td></tr><tr><td>eval/samples_per_second</td><td>118.826</td></tr><tr><td>eval/steps_per_second</td><td>3.716</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>4320</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5582</td></tr><tr><td>train/total_flos</td><td>3303100032860160.0</td></tr><tr><td>train/train_loss</td><td>0.59298</td></tr><tr><td>train/train_runtime</td><td>3029.514</td></tr><tr><td>train/train_samples_per_second</td><td>45.623</td></tr><tr><td>train/train_steps_per_second</td><td>1.426</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">albert-base-v2-bs32-task-split</strong>: <a href=\"https://wandb.ai/wschella/lass/runs/3063kkvs\" target=\"_blank\">https://wandb.ai/wschella/lass/runs/3063kkvs</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220615_124502-3063kkvs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer.train()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "406d51ce-e5c8-4c44-958e-c12d1e4818c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce8153-b7de-46ff-9457-fcf1f1e58090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eed4bfcf3d3cfcdb00482c10052e8eba5705b015008b357326d56e176b5397df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
