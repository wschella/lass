{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504614d5",
   "metadata": {
    "papermill": {
     "duration": 0.004838,
     "end_time": "2022-06-29T13:56:37.850531",
     "exception": false,
     "start_time": "2022-06-29T13:56:37.845693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explainable AI applied to assessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f6b76b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T13:56:37.859658Z",
     "iopub.status.busy": "2022-06-29T13:56:37.859124Z",
     "iopub.status.idle": "2022-06-29T13:56:42.001326Z",
     "shell.execute_reply": "2022-06-29T13:56:41.999919Z"
    },
    "papermill": {
     "duration": 4.150531,
     "end_time": "2022-06-29T13:56:42.004948",
     "exception": false,
     "start_time": "2022-06-29T13:56:37.854417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers.models.auto.modeling_auto import AutoModelForSequenceClassification\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bigbench.api.results as bb\n",
    "\n",
    "from lass.log_handling import LogLoader\n",
    "from lass.datasets import split_instance_level, huggingfaceify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e09d79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T13:56:42.014595Z",
     "iopub.status.busy": "2022-06-29T13:56:42.013756Z"
    },
    "papermill": {
     "duration": 1.32557,
     "end_time": "2022-06-29T13:56:43.334440",
     "exception": false,
     "start_time": "2022-06-29T13:56:42.008870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "loader = (LogLoader(logdir = Path('../artifacts/logs'))\n",
    "        .with_tasks('paper-full')\n",
    "        .with_model_families(['BIG-G T=0'])\n",
    "        .with_model_sizes(['128b'])\n",
    "        .with_shots([0])\n",
    "        .with_query_types([bb.MultipleChoiceQuery])\n",
    ")\n",
    "\n",
    "_train, test = split_instance_level(loader, seed=42, test_fraction=0.2)\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3729a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T13:53:31.594285Z",
     "iopub.status.busy": "2022-06-29T13:53:31.593936Z",
     "iopub.status.idle": "2022-06-29T13:53:31.650013Z",
     "shell.execute_reply": "2022-06-29T13:53:31.649128Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "dataset = huggingfaceify(_train[:1], test)\n",
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55713b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T13:53:31.664340Z",
     "iopub.status.busy": "2022-06-29T13:53:31.663958Z",
     "iopub.status.idle": "2022-06-29T13:53:41.210720Z",
     "shell.execute_reply": "2022-06-29T13:53:41.209619Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error() # type: ignore\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../artifacts/assessors/bert-bs32-0sh/checkpoint-1500\")\n",
    "\n",
    "# Tokenize according to specific model tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# This \"transformer-interpret\" library doesn't really deal well with truncation\n",
    "# of long sequences, so we'll just truncate the sequences ourselves.\n",
    "# \n",
    "# Note: Encoding is a destructive process,\n",
    "# so we need to work with offset_mapping instead of just reversing.\n",
    "# https://github.com/huggingface/tokenizers/issues/826#issuecomment-966082496\n",
    "def truncate(batch):\n",
    "   tokens = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, \n",
    "      return_tensors=\"np\", return_offsets_mapping=True)\n",
    "\n",
    "   # We use `amax` as a trick to find the last non-zero offset mapping.\n",
    "   # Array dimensions are [batch size, sequence length (in tokens), 2],\n",
    "   # where the last dimension is [start, end] of the token (referring to index in the string).\n",
    "   # With [:,:,-1], we make it [batch size, sequence length], taking only the end offset of the token.\n",
    "   # Then we take the max of each sequence, producing batch_size numbers.\n",
    "   lengths = np.amax(tokens['offset_mapping'][:,:,-1], axis=1) # type: ignore\n",
    "   assert len(lengths) == len(batch['text'])\n",
    "\n",
    "   # Now we cut all the strings\n",
    "   texts = [text[:end] for text, end in zip(batch[\"text\"], lengths)]\n",
    "   return {'text': texts, 'label': batch['label']}\n",
    "\n",
    "truncated_datasets = dataset.map(truncate, batched=True)\n",
    "truncated_datasets['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a9e71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T13:53:41.217328Z",
     "iopub.status.busy": "2022-06-29T13:53:41.216734Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "from collections import defaultdict\n",
    "\n",
    "cls_explainer = SequenceClassificationExplainer(model, tokenizer) #type: ignore\n",
    "\n",
    "# Print header\n",
    "path = Path(\"xai.csv\")\n",
    "pd.DataFrame([],columns=['word', 'contribution', 'LM_score', 'Assr_pred']).to_csv(path, index=False)\n",
    "\n",
    "# for index, instance in enumerate(truncated_datasets['test'].select(range(255, 260))):\n",
    "for index, instance in enumerate(truncated_datasets['test']):\n",
    "    if index % 50 == 0: # type: ignore\n",
    "        print(f\"{index}/{len(truncated_datasets['test'])}\")\n",
    "\n",
    "    # EXPLAINABILITY\n",
    "    text, LM_correct = instance['text'], instance['label'] # type: ignore\n",
    "    exp_neg = cls_explainer(text, class_name='LABEL_0')\n",
    "\n",
    "    frame = pd.DataFrame(exp_neg, columns=['word', 'contribution'])\n",
    "    frame['LM_score'] = LM_correct\n",
    "    frame['Assr_pred'] = cls_explainer.predicted_class_index\n",
    "    frame.to_csv(path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b48d0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# frame.groupby('word').agg( # type: ignore\n",
    "#     mean_contribution=('contribution', 'mean'),\n",
    "#     word_count=('word', 'count')\n",
    "# ).sort_values('mean_contribution', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.407087,
   "end_time": "2022-06-29T13:56:44.355928",
   "environment_variables": {},
   "exception": null,
   "input_path": "Exp_XAI.ipynb",
   "output_path": "Exp_XAI.ipynb",
   "parameters": {},
   "start_time": "2022-06-29T13:56:36.948841",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "eed4bfcf3d3cfcdb00482c10052e8eba5705b015008b357326d56e176b5397df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}