{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-05-09 17:45:59][INFO][test/test]: Starting data loading\n",
      "[2023-05-09 17:45:59][INFO][test/test]: Loaded data.\n",
      "[2023-05-09 17:45:59][INFO][pipeline/binarize]: Dropped 0 samples with non-binary correctness\n",
      "[2023-05-09 17:45:59][INFO][pipeline/clean]: Dropped 0 samples with faulty targets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metrics': {'conf-absolute': {'acc': {'test': 0.5707317073170731,\n",
      "                                       'train': 0.6227106227106227},\n",
      "                               'balanced_acc': {'test': 0.4980457299198749,\n",
      "                                                'train': 0.5102842809364548},\n",
      "                               'bs': {'test': 0.2519547002361981,\n",
      "                                      'train': 0.23497861006628068},\n",
      "                               'bs_dcr': {'test': 0.0030622411188156173,\n",
      "                                          'train': 0.00284925499537364},\n",
      "                               'bs_mcb': {'test': 0.011495228089100556,\n",
      "                                          'train': 0.006031442789041663},\n",
      "                               'bs_unc': {'test': 0.24352171326591315,\n",
      "                                          'train': 0.23179642227261266},\n",
      "                               'roc_auc': {'test': 0.5027359781121752,\n",
      "                                           'train': 0.5334769745304863}},\n",
      "             'conf-normalized': {'acc': {'test': 0.4195121951219512,\n",
      "                                         'train': 0.36507936507936506},\n",
      "                                 'balanced_acc': {'test': 0.5, 'train': 0.5},\n",
      "                                 'bs': {'test': 0.3189076041033631,\n",
      "                                        'train': 0.3429097517480981},\n",
      "                                 'bs_dcr': {'test': 0.011593965732579231,\n",
      "                                            'train': 0.002219315398245242},\n",
      "                                 'bs_mcb': {'test': 0.08697985657002918,\n",
      "                                            'train': 0.11333264487373071},\n",
      "                                 'bs_unc': {'test': 0.24352171326591315,\n",
      "                                            'train': 0.23179642227261266},\n",
      "                                 'roc_auc': {'test': 0.53156146179402,\n",
      "                                             'train': 0.5287947002829946}},\n",
      "             'task-acc': {'test': 0.4195121951219512,\n",
      "                          'train': 0.36507936507936506}},\n",
      " 'stats': {'n_instances': {'test': 205, 'train': 819},\n",
      "           'n_instances_nonbinary': {'test': 0, 'train': 0},\n",
      "           'n_tasks': {'test': 1, 'train': 1}}}\n",
      "                                                 input  \\\n",
      "525  Identify the relation between the following pr...   \n",
      "\n",
      "                          targets                                    scores  \\\n",
      "525  [entailment, non-entailment]  [-0.7064244747161865, -2.01547908782959]   \n",
      "\n",
      "                              target_values  correct  \\\n",
      "525  {'entailment': 1, 'non-entailment': 0}        1   \n",
      "\n",
      "                              absolute_scores  \\\n",
      "525  [-0.7064244747161865, -2.01547908782959]   \n",
      "\n",
      "                              normalized_scores  \\\n",
      "525  [-0.23907619714736938, -1.548130750656128]   \n",
      "\n",
      "                                               metrics model_name  \\\n",
      "525  {'calibration_multiple_choice_brier_score': 0....       128b   \n",
      "\n",
      "    model_family                 task  shots  n_targets  conf_normalized  \\\n",
      "525    BIG-G T=0  epistemic_reasoning      0          2         0.787355   \n",
      "\n",
      "     conf_absolute  \n",
      "525       0.493405  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wout/Code/pp/lass/.env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /home/wout/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/spm.model from cache at /home/wout/.cache/huggingface/transformers/ec748fd4f03d0e5a2d5d56dff01e6dd733f23c67105cd54a9910f9d711870253.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer_config.json from cache at /home/wout/.cache/huggingface/transformers/967a4d63eb35950cfd24a9e335906419009f32940fa2ba1b73e7ba032628c38d.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n",
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /home/wout/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Adding [MASK] to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /home/wout/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "/home/wout/Code/pp/lass/.env/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[2023-05-09 17:46:04][WARNING][fingerprint/update_fingerprint]: Parameter 'function'=<function tokenize.<locals>.tokenize_function at 0x7f60c0987ac0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942d8fcc58e444aea85eea9f955d8ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/26 00:00 < 00:02, 9.30 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wout/Code/pp/lass/.env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "[2023-05-09 17:46:08][INFO][test/test]: Starting tokenization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\n\\nPremise: Isabella remembers that Olivia believes that two men are working together to empty the barrel of its contents. Hypothesis: Olivia believes that Isabella remembers that two men are working together to empty the barrel of its contents.\\nRelation: \", 'label': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /home/wout/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/spm.model from cache at /home/wout/.cache/huggingface/transformers/ec748fd4f03d0e5a2d5d56dff01e6dd733f23c67105cd54a9910f9d711870253.0abaeacf7287ee8ba758fec15ddfb4bb6c697bb1a8db272725f8aa633501787a\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/tokenizer_config.json from cache at /home/wout/.cache/huggingface/transformers/967a4d63eb35950cfd24a9e335906419009f32940fa2ba1b73e7ba032628c38d.df5a7f41459442f66bec27ac9352bba694cde109855024b3ae61be2f5734ee9a\n",
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /home/wout/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Adding [MASK] to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file https://huggingface.co/microsoft/deberta-v3-base/resolve/main/config.json from cache at /home/wout/.cache/huggingface/transformers/e6f9db57345f0f60c9f837fa97bcb27b1ed31e99feb33d732d7d8c80cb8f8459.de97182a9f32a68819030ba8f3f6ff2ba47276be3864425925523202f54cc79c\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "/home/wout/Code/pp/lass/.env/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92da1c9976a40c08ec3a6aeac6ff29f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n",
      "***** Running Prediction *****\n",
      "  Num examples = 205\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# autopep8: off\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "from lass.test import test\n",
    "from lass.log_handling import LogLoaderArgs\n",
    "# autopep8: on\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='[%(asctime)s][%(levelname)s][%(module)s/%(funcName)s]: %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    return test(\n",
    "        data_args=LogLoaderArgs(\n",
    "            logdir=\"../artifacts/logs\",\n",
    "            # tasks=\"paper-full\",\n",
    "            tasks=[\"epistemic_reasoning\"],\n",
    "            model_families=[\"BIG-G T=0\"],\n",
    "            model_sizes=[\"128b\"],\n",
    "            # model_sizes=[\"2m\"],\n",
    "            shots=[0],\n",
    "            query_types=[\"multiple_choice\"],\n",
    "        ),\n",
    "        model_name=\"microsoft/deberta-v3-base\",\n",
    "        split=\"instance\",\n",
    "        model_loc=\"../artifacts/assessors/reference-models/deberta-reference-bs16*2-0sh-instance-split-10241537/checkpoint-4000\",\n",
    "        per_task=True,\n",
    "    )\n",
    "\n",
    "results = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: pd.DataFrame = results[\"test\"] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>targets</th>\n",
       "      <th>scores</th>\n",
       "      <th>target_values</th>\n",
       "      <th>correct</th>\n",
       "      <th>absolute_scores</th>\n",
       "      <th>normalized_scores</th>\n",
       "      <th>metrics</th>\n",
       "      <th>model_name</th>\n",
       "      <th>model_family</th>\n",
       "      <th>task</th>\n",
       "      <th>shots</th>\n",
       "      <th>n_targets</th>\n",
       "      <th>conf_normalized</th>\n",
       "      <th>conf_absolute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Identify the relation between the following pr...</td>\n",
       "      <td>[entailment, non-entailment]</td>\n",
       "      <td>[-1.1246986389160156, -1.6564757823944092]</td>\n",
       "      <td>{'entailment': 0, 'non-entailment': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1.1246986389160156, -1.6564757823944092]</td>\n",
       "      <td>[-0.4621981978416443, -0.9939753413200378]</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "      <td>128b</td>\n",
       "      <td>BIG-G T=0</td>\n",
       "      <td>epistemic_reasoning</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.629897</td>\n",
       "      <td>0.324750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Identify the relation between the following pr...</td>\n",
       "      <td>[entailment, non-entailment]</td>\n",
       "      <td>[-1.260854959487915, -1.810600757598877]</td>\n",
       "      <td>{'entailment': 1, 'non-entailment': 0}</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.260854959487915, -1.810600757598877]</td>\n",
       "      <td>[-0.4555854797363281, -1.00533127784729]</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "      <td>128b</td>\n",
       "      <td>BIG-G T=0</td>\n",
       "      <td>epistemic_reasoning</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.634077</td>\n",
       "      <td>0.283412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Identify the relation between the following pr...</td>\n",
       "      <td>[entailment, non-entailment]</td>\n",
       "      <td>[-1.130898118019104, -1.812220811843872]</td>\n",
       "      <td>{'entailment': 0, 'non-entailment': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1.130898118019104, -1.812220811843872]</td>\n",
       "      <td>[-0.4094221591949463, -1.0907448530197144]</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "      <td>128b</td>\n",
       "      <td>BIG-G T=0</td>\n",
       "      <td>epistemic_reasoning</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.664034</td>\n",
       "      <td>0.322743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Identify the relation between the following pr...</td>\n",
       "      <td>[entailment, non-entailment]</td>\n",
       "      <td>[-1.098881721496582, -1.9925997257232666]</td>\n",
       "      <td>{'entailment': 1, 'non-entailment': 0}</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.098881721496582, -1.9925997257232666]</td>\n",
       "      <td>[-0.3429737091064453, -1.2366917133331299]</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "      <td>128b</td>\n",
       "      <td>BIG-G T=0</td>\n",
       "      <td>epistemic_reasoning</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.709657</td>\n",
       "      <td>0.333244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Identify the relation between the following pr...</td>\n",
       "      <td>[entailment, non-entailment]</td>\n",
       "      <td>[-0.9820577502250671, -2.104311466217041]</td>\n",
       "      <td>{'entailment': 0, 'non-entailment': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.9820577502250671, -2.104311466217041]</td>\n",
       "      <td>[-0.28182393312454224, -1.4040776491165161]</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "      <td>128b</td>\n",
       "      <td>BIG-G T=0</td>\n",
       "      <td>epistemic_reasoning</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.754406</td>\n",
       "      <td>0.374540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Identify the relation between the following pr...</td>\n",
       "      <td>[entailment, non-entailment]</td>\n",
       "      <td>[-1.0202032327651978, -1.6574050188064575]</td>\n",
       "      <td>{'entailment': 0, 'non-entailment': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1.0202032327651978, -1.6574050188064575]</td>\n",
       "      <td>[-0.42446351051330566, -1.0616652965545654]</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "      <td>128b</td>\n",
       "      <td>BIG-G T=0</td>\n",
       "      <td>epistemic_reasoning</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.654121</td>\n",
       "      <td>0.360522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>Identify the relation between the following pr...</td>\n",
       "      <td>[entailment, non-entailment]</td>\n",
       "      <td>[-0.9340126514434814, -1.7481417655944824]</td>\n",
       "      <td>{'entailment': 0, 'non-entailment': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.9340126514434814, -1.7481417655944824]</td>\n",
       "      <td>[-0.3667415976524353, -1.180870771408081]</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "      <td>128b</td>\n",
       "      <td>BIG-G T=0</td>\n",
       "      <td>epistemic_reasoning</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.692989</td>\n",
       "      <td>0.392974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>Identify the relation between the following pr...</td>\n",
       "      <td>[entailment, non-entailment]</td>\n",
       "      <td>[-0.7094211578369141, -1.9706110954284668]</td>\n",
       "      <td>{'entailment': 0, 'non-entailment': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.7094211578369141, -1.9706110954284668]</td>\n",
       "      <td>[-0.24944794178009033, -1.510637879371643]</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "      <td>128b</td>\n",
       "      <td>BIG-G T=0</td>\n",
       "      <td>epistemic_reasoning</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.779231</td>\n",
       "      <td>0.491929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>Identify the relation between the following pr...</td>\n",
       "      <td>[entailment, non-entailment]</td>\n",
       "      <td>[-1.3657355308532715, -2.004938840866089]</td>\n",
       "      <td>{'entailment': 1, 'non-entailment': 0}</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.3657355308532715, -2.004938840866089]</td>\n",
       "      <td>[-0.42377161979675293, -1.0629749298095703]</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "      <td>128b</td>\n",
       "      <td>BIG-G T=0</td>\n",
       "      <td>epistemic_reasoning</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.654573</td>\n",
       "      <td>0.255193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>Identify the relation between the following pr...</td>\n",
       "      <td>[entailment, non-entailment]</td>\n",
       "      <td>[-0.8847140073776245, -2.151432752609253]</td>\n",
       "      <td>{'entailment': 0, 'non-entailment': 1}</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.8847140073776245, -2.151432752609253]</td>\n",
       "      <td>[-0.24822992086410522, -1.5149486064910889]</td>\n",
       "      <td>{'calibration_multiple_choice_brier_score': 0....</td>\n",
       "      <td>128b</td>\n",
       "      <td>BIG-G T=0</td>\n",
       "      <td>epistemic_reasoning</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.780181</td>\n",
       "      <td>0.412832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input  \\\n",
       "1     Identify the relation between the following pr...   \n",
       "4     Identify the relation between the following pr...   \n",
       "13    Identify the relation between the following pr...   \n",
       "14    Identify the relation between the following pr...   \n",
       "20    Identify the relation between the following pr...   \n",
       "...                                                 ...   \n",
       "1000  Identify the relation between the following pr...   \n",
       "1004  Identify the relation between the following pr...   \n",
       "1006  Identify the relation between the following pr...   \n",
       "1016  Identify the relation between the following pr...   \n",
       "1020  Identify the relation between the following pr...   \n",
       "\n",
       "                           targets  \\\n",
       "1     [entailment, non-entailment]   \n",
       "4     [entailment, non-entailment]   \n",
       "13    [entailment, non-entailment]   \n",
       "14    [entailment, non-entailment]   \n",
       "20    [entailment, non-entailment]   \n",
       "...                            ...   \n",
       "1000  [entailment, non-entailment]   \n",
       "1004  [entailment, non-entailment]   \n",
       "1006  [entailment, non-entailment]   \n",
       "1016  [entailment, non-entailment]   \n",
       "1020  [entailment, non-entailment]   \n",
       "\n",
       "                                          scores  \\\n",
       "1     [-1.1246986389160156, -1.6564757823944092]   \n",
       "4       [-1.260854959487915, -1.810600757598877]   \n",
       "13      [-1.130898118019104, -1.812220811843872]   \n",
       "14     [-1.098881721496582, -1.9925997257232666]   \n",
       "20     [-0.9820577502250671, -2.104311466217041]   \n",
       "...                                          ...   \n",
       "1000  [-1.0202032327651978, -1.6574050188064575]   \n",
       "1004  [-0.9340126514434814, -1.7481417655944824]   \n",
       "1006  [-0.7094211578369141, -1.9706110954284668]   \n",
       "1016   [-1.3657355308532715, -2.004938840866089]   \n",
       "1020   [-0.8847140073776245, -2.151432752609253]   \n",
       "\n",
       "                               target_values  correct  \\\n",
       "1     {'entailment': 0, 'non-entailment': 1}        0   \n",
       "4     {'entailment': 1, 'non-entailment': 0}        1   \n",
       "13    {'entailment': 0, 'non-entailment': 1}        0   \n",
       "14    {'entailment': 1, 'non-entailment': 0}        1   \n",
       "20    {'entailment': 0, 'non-entailment': 1}        0   \n",
       "...                                      ...      ...   \n",
       "1000  {'entailment': 0, 'non-entailment': 1}        0   \n",
       "1004  {'entailment': 0, 'non-entailment': 1}        0   \n",
       "1006  {'entailment': 0, 'non-entailment': 1}        0   \n",
       "1016  {'entailment': 1, 'non-entailment': 0}        1   \n",
       "1020  {'entailment': 0, 'non-entailment': 1}        0   \n",
       "\n",
       "                                 absolute_scores  \\\n",
       "1     [-1.1246986389160156, -1.6564757823944092]   \n",
       "4       [-1.260854959487915, -1.810600757598877]   \n",
       "13      [-1.130898118019104, -1.812220811843872]   \n",
       "14     [-1.098881721496582, -1.9925997257232666]   \n",
       "20     [-0.9820577502250671, -2.104311466217041]   \n",
       "...                                          ...   \n",
       "1000  [-1.0202032327651978, -1.6574050188064575]   \n",
       "1004  [-0.9340126514434814, -1.7481417655944824]   \n",
       "1006  [-0.7094211578369141, -1.9706110954284668]   \n",
       "1016   [-1.3657355308532715, -2.004938840866089]   \n",
       "1020   [-0.8847140073776245, -2.151432752609253]   \n",
       "\n",
       "                                normalized_scores  \\\n",
       "1      [-0.4621981978416443, -0.9939753413200378]   \n",
       "4        [-0.4555854797363281, -1.00533127784729]   \n",
       "13     [-0.4094221591949463, -1.0907448530197144]   \n",
       "14     [-0.3429737091064453, -1.2366917133331299]   \n",
       "20    [-0.28182393312454224, -1.4040776491165161]   \n",
       "...                                           ...   \n",
       "1000  [-0.42446351051330566, -1.0616652965545654]   \n",
       "1004    [-0.3667415976524353, -1.180870771408081]   \n",
       "1006   [-0.24944794178009033, -1.510637879371643]   \n",
       "1016  [-0.42377161979675293, -1.0629749298095703]   \n",
       "1020  [-0.24822992086410522, -1.5149486064910889]   \n",
       "\n",
       "                                                metrics model_name  \\\n",
       "1     {'calibration_multiple_choice_brier_score': 0....       128b   \n",
       "4     {'calibration_multiple_choice_brier_score': 0....       128b   \n",
       "13    {'calibration_multiple_choice_brier_score': 0....       128b   \n",
       "14    {'calibration_multiple_choice_brier_score': 0....       128b   \n",
       "20    {'calibration_multiple_choice_brier_score': 0....       128b   \n",
       "...                                                 ...        ...   \n",
       "1000  {'calibration_multiple_choice_brier_score': 0....       128b   \n",
       "1004  {'calibration_multiple_choice_brier_score': 0....       128b   \n",
       "1006  {'calibration_multiple_choice_brier_score': 0....       128b   \n",
       "1016  {'calibration_multiple_choice_brier_score': 0....       128b   \n",
       "1020  {'calibration_multiple_choice_brier_score': 0....       128b   \n",
       "\n",
       "     model_family                 task  shots  n_targets  conf_normalized  \\\n",
       "1       BIG-G T=0  epistemic_reasoning      0          2         0.629897   \n",
       "4       BIG-G T=0  epistemic_reasoning      0          2         0.634077   \n",
       "13      BIG-G T=0  epistemic_reasoning      0          2         0.664034   \n",
       "14      BIG-G T=0  epistemic_reasoning      0          2         0.709657   \n",
       "20      BIG-G T=0  epistemic_reasoning      0          2         0.754406   \n",
       "...           ...                  ...    ...        ...              ...   \n",
       "1000    BIG-G T=0  epistemic_reasoning      0          2         0.654121   \n",
       "1004    BIG-G T=0  epistemic_reasoning      0          2         0.692989   \n",
       "1006    BIG-G T=0  epistemic_reasoning      0          2         0.779231   \n",
       "1016    BIG-G T=0  epistemic_reasoning      0          2         0.654573   \n",
       "1020    BIG-G T=0  epistemic_reasoning      0          2         0.780181   \n",
       "\n",
       "      conf_absolute  \n",
       "1          0.324750  \n",
       "4          0.283412  \n",
       "13         0.322743  \n",
       "14         0.333244  \n",
       "20         0.374540  \n",
       "...             ...  \n",
       "1000       0.360522  \n",
       "1004       0.392974  \n",
       "1006       0.491929  \n",
       "1016       0.255193  \n",
       "1020       0.412832  \n",
       "\n",
       "[205 rows x 15 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_preds = np.array(df.scores.values.tolist())\n",
    "subject_preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0, 'non-entailment': 1}    118\n",
       "{'entailment': 1, 'non-entailment': 0}      1\n",
       "Name: target_values, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"correct == 0\").target_values.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 1, 'non-entailment': 0}    85\n",
       "{'entailment': 0, 'non-entailment': 1}     1\n",
       "Name: target_values, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"correct == 1\").target_values.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>asss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     correct  asss\n",
       "58         0     1\n",
       "315        1     0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"logits\"].argmax(axis=1)\n",
    "\n",
    "j = df[\"correct\"].to_frame()\n",
    "j[\"asss\"] = results[\"logits\"].argmax(axis=1)\n",
    "j.query(\"correct != asss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.037272121757268906,\n",
       " 'test_conf_distribution_accuracy': 0.5804878048780487,\n",
       " 'test_conf_distribution_precision': 0.0,\n",
       " 'test_conf_distribution_recall': 0.0,\n",
       " 'test_conf_distribution_f1': 0.0,\n",
       " 'test_conf_distribution_roc_auc': 0.5,\n",
       " 'test_conf_distribution_bs': 0.24352171326591315,\n",
       " 'test_conf_distribution_bs_mcb': 0.0,\n",
       " 'test_conf_distribution_bs_dsc': 0.0,\n",
       " 'test_conf_distribution_bs_unc': 0.24352171326591315,\n",
       " 'test_conf_distribution_balanced_accuracy': 0.5,\n",
       " 'test_conf_absolute_accuracy': 0.5707317073170731,\n",
       " 'test_conf_absolute_precision': 0.4,\n",
       " 'test_conf_absolute_recall': 0.046511627906976744,\n",
       " 'test_conf_absolute_f1': 0.08333333333333333,\n",
       " 'test_conf_absolute_roc_auc': 0.5027359781121752,\n",
       " 'test_conf_absolute_bs': 0.2519547002361981,\n",
       " 'test_conf_absolute_bs_mcb': 0.011495228089100556,\n",
       " 'test_conf_absolute_bs_dsc': 0.0030622411188156173,\n",
       " 'test_conf_absolute_bs_unc': 0.24352171326591315,\n",
       " 'test_conf_absolute_balanced_accuracy': 0.4980457299198749,\n",
       " 'test_conf_normalized_accuracy': 0.4195121951219512,\n",
       " 'test_conf_normalized_precision': 0.4195121951219512,\n",
       " 'test_conf_normalized_recall': 1.0,\n",
       " 'test_conf_normalized_f1': 0.5910652920962199,\n",
       " 'test_conf_normalized_roc_auc': 0.53156146179402,\n",
       " 'test_conf_normalized_bs': 0.3189076041033631,\n",
       " 'test_conf_normalized_bs_mcb': 0.08697985657002918,\n",
       " 'test_conf_normalized_bs_dsc': 0.011593965732579231,\n",
       " 'test_conf_normalized_bs_unc': 0.24352171326591315,\n",
       " 'test_conf_normalized_balanced_accuracy': 0.5,\n",
       " 'test_accuracy': 0.9902439024390244,\n",
       " 'test_precision': 0.9883720930232558,\n",
       " 'test_recall': 0.9883720930232558,\n",
       " 'test_f1': 0.9883720930232558,\n",
       " 'test_roc_auc': 0.9997068594879812,\n",
       " 'test_bs': 0.008541745723370688,\n",
       " 'test_bs_mcb': 0.0048832091380048345,\n",
       " 'test_bs_dsc': 0.2398631766805473,\n",
       " 'test_bs_unc': 0.24352171326591315,\n",
       " 'test_balanced_accuracy': 0.989984365839359,\n",
       " 'test_runtime': 2.8096,\n",
       " 'test_samples_per_second': 72.965,\n",
       " 'test_steps_per_second': 9.254}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"metrics\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
