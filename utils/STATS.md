# BIG-bench Exploration

This file contains some statistics about the evaluation data generated by BIG-bench.

- Number of tasks:

  - 209 through `ls -d bigbench/benchmark_tasks/*/ | wc -l`

  - 208 through `ls bigbench/benchmark_tasks/**/results/full-logs** | wc -l`

    We miss:

    - `contextual_parametric_knowledge_conflicts`.

    This one was likely merged too late to be discussed in first results.
    Differences can be found through:

    ```shell
    diff \
    <(ls -d bigbench/benchmark_tasks/*/) \
    <(ls bigbench/benchmark_tasks/**/results/full-logs** | xargs dirname | xargs dirname | xargs -I {} echo {}"/")
    ```

  - 207 mentioned in the paper. Likely excludes `training_on_test_set` as well.

- 1.1G total of compressed evaluation data
  (ordered by size + total)  
  `du -csh $(ls -S bigbench/benchmark_tasks/**/results/full-logs**)`
- 57G total of uncompressed evaluation data  
  `du -sh artifacts/logs/`
- 3 additional tasks without Google data, thus without instance level results.  
  `du -sh artifacts/logs/* | sort -hr`

  - `simple_arithmetic`
  - `long_context_integration`
  - `convinceme`

- Number of samples

  - 222668 multiple choice samples across 1356 queries, per model. Estimated by counting them for a single model (BIG-G T=0 2m), and assuming it's the same for the rest.
  - 55667 zero-shot multiple choice across samples 339 queries, per model

  - Long sequences (over 1024 BERT tokens)

    ```txt
    0-shot: 5125/124349 long sequences
    1-shot: 10152/124349 long sequences
    2-shot: 18729/124349 long sequences
    3-shot: 26596/124349 long sequences
    Unknown-shots: 1297/24006 long sequences
    ```

  - Long sequences (over 1024 BERT tokens) (Multiple Choice)

    ```txt
    0-shot MCQ: 2483/55431 long sequences
    1-shot MCQ: 3604/55431 long sequences
    2-shot MCQ: 5889/55431 long sequences
    3-shot MCQ: 8514/55431 long sequences
    Unknown-shots: 0/0 long sequences
    ```
