# BIG-bench Exploration

This file contains some statistics about the evaluation data generated by BIG-bench.

- Number of tasks:

  - 209 through `ls -d bigbench/benchmark_tasks/*/ | wc -l`

  - 208 through `ls bigbench/benchmark_tasks/**/results/full-logs** | wc -l`

    We miss:

    - `contextual_parametric_knowledge_conflicts`.

    This one was likely merged too late to be discussed in first results.
    Differences can be found through:

    ```shell
    diff \
    <(ls -d bigbench/benchmark_tasks/*/) \
    <(ls bigbench/benchmark_tasks/**/results/full-logs** | xargs dirname | xargs dirname | xargs -I {} echo {}"/")
    ```

  - 207 mentioned in the paper. Likely excludes `training_on_test_set` as well.

- 1.1G total of compressed evaluation data
  (ordered by size + total)  
  `du -csh $(ls -S bigbench/benchmark_tasks/**/results/full-logs**)`
- 57G total of uncompressed evaluation data  
  `du -sh artifacts/logs/`
- 3 additional tasks without Google data, thus without instance level results.  
  `du -sh artifacts/logs/* | sort -hr`

  - `simple_arithmetic`
  - `long_context_integration`
  - `convinceme`

- Number of samples
  - 222668 multiple choice samples across 1356 queries, per model. Estimated by counting them for a single model (BIG-G T=0 2m), and assuming it's the same for the rest.
  - 55667 zero-shot multiple choice across samples 339 queries, per model
